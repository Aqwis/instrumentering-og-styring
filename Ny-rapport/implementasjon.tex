\subsection{Bildegjenkjenning}

Kongsberg Gruppen la klare retningslinjer om at OpenCV skulle benyttes. I tillegg oppfordret Kongsberg Gruppen til å implementere programvaren i programmeringsspråket Matlab, ettersom de internt bruker Matlab til prototyping.

I prosjektets begynnende fase var siste stabile utgivelse av OpenCV versjon 2.4.8, som ikke støttet programmering i Matlab. For å bruke Matlab og OpenCV sammen gjensto det da to alternativer. Versjon 3.0 av OpenCV, som var under utvikling, støttet Matlab. I tillegg fantes det et tredjeparts kompatibilitetslag, \emph{mexopencv}, som gjorde det mulig å programmere med OpenCV i Matlab.

Begge muligheter ble undersøkt. mexopencv viste seg å være problematisk å kompilere på de ulike kombinasjonene av operativsystemer og Matlab-versjoner gruppedmedlemmene satt med. Videre hadde mexopencv problemer med grafikkortene som ble benyttet på gruppens datamaskiner. Utviklingsversjonen av OpenCV, på sin side, ville overhodet ikke kompilere. Antakelig skyldes dette at kodebasen for denne versjonen var under rask utvikling og dermed ustabil. Etter å ikke ha fått Matlab og OpenCV til å fungere sammen, ble det valgt å heller implementere bildeprosesseringsalgoritmene i C++. Kongsberg ble informert om dette, og var fornøyd med løsningen.

\subsubsection{Innhenting av bilde og konvertering}

Kameraet ga ut bilder i fargespekteret RGB, som fungerer godt for bilderedigering i forbrukerprogrmavare som Photoshop, men som er lite egnet til bildeprosesseringsformål. Valget falt dermed først på å heller benytte fargespekteret HSV, som er hyppig brukt innen bildeprosessering og datasyn. Hvert bilde i videoen ble konvertert fra RGB til HSV før det ble prosessert videre. I figur [\ref{fig:firstiterationrgb}] og figur [\ref{fig:firstiterationhsv}] er det samme bildet vist henholdsvis før og etter konvertering fra RGB til HSV.

\begin{figure}[!ht]
	\centering
	\subcaptionbox{RGB\label{fig:firstiterationrgb}}
	{\includegraphics[width=0.45\textwidth]{img/first-rgb.jpg}}
	\subcaptionbox{HSV\label{fig:firstiterationhsv}}
	{\includegraphics[width=0.45\textwidth]{img/first-hsv.jpg}}
	\caption{Bilde fra kamera i RGB og HSV}
\end{figure}

Å konvertere bildet til HSV før prosessering fungerte tilfredsstillende for tilfeller med stor fargekontrast mellom objekt og bakgrunn. I mer kaotiske scener oppstod det derimot problemer. For å kunne tracke objekter selv om de for eksempel ble beveget fra områder med sterkt lys til områder med skygge, var løsingen å øke avstanden mellom maksimums- og minimumsverdiene for value-aksen. Dette førte imidlertid til at objekter av andre farger også ble plukket opp. Årsaken til dette er at HSV ikke samsvarer helt med måten mennesker oppfatter farger på, så endringer på value-aksen korresponderer ikke helt med det som skjer med fargen til et objekt når den blir truffet av skygge eller lys. Et annet problem med HSV var at objekter med svakere farger ble plukket opp dårligere.

\begin{figure}[!ht]
	\centering
	\subcaptionbox{RGB\label{fig:rgblab1}}
	{\includegraphics[width=0.45\textwidth]{img/third-rgb.jpg}}
	\subcaptionbox{LAB\label{fig:rgblab2}}
	{\includegraphics[width=0.45\textwidth]{img/third-lab.jpg}}
	\caption{Bilde fra kamera i RGB og LAB}
\end{figure}

Det ble eksperimentert med flere ulike alternativer til HSV. Av alle alternativene fungerte LAB best, mer spesifikt CIE L*a*b*-standarden. En LAB-farge består av tre akser -- \emph{Lightness} og to fargespesiserende akser, \emph{a} og \emph{b}. LAB-fargerommet er laget for å korrespondere bedre med måten mennesker oppfatter farger på. Å beskrive nøyaktig hvorfor LAB fungerte bedre enn HSV for det aktuelle formålet er utenfor målet med denne oppgaven, men erfaringen tilsier at LAB tillot bedre deteksjon og følging av objekter av mange ulike farger inn og ut av skyggeområder. Valget falt dermed på LAB som erstatning for HSV. Et bilde fra kameraet i RGB og det samme bildet konvertert til LAB er vist i henholdsvis figur [\ref{fig:rgblab1}] og figur [\ref{fig:rgblab2}].

\subsubsection{Deteksjon}

\begin{wrapfigure}[12]{r}{0.25\textwidth}
	\includegraphics[width=\linewidth]{img/sliders.jpg}
	\caption{Slidere for å velge HSV-verdier}
	\label{fig:sliders}
\end{wrapfigure}

Etter å ha konvertert bildet til HSV, ble bildematrisen behandlet med et filter som manuelt ble stilt inn med maksimums- og minimumsverdier for \emph{hue}, \emph{saturation} og \emph{value}, som vist i figur [\ref{fig:sliders}]. Resultatet av dette var et binært bilde, der fargene med H-, S-, og V-verdier mellom disse maksimums- og minimumsverdiene er gjengitt i hvitt, mens alle andre farger er gjengitt i svart, som vist i figur [\ref{fig:firstiterationbinary}].

Neste steg var nå å velge ut ett objekt fra bildet slik at det kunne spores. Definisjonen av et \emph{objekt} er her et sammenhengende hvitt område i det binære bildet. Metoden som ble valgt var å velge ut det største objektet, og returnere dets midtpunkt. Dersom det er for mye støy i bildet, det vil si for mange ulike objekter, returnerer algoritmen at objektet ikke ble funnet.

\subsubsection{Smoothing}
Det var nå mulig å detektere et valgt objekt og returnere koordinatene til dette objektet, men som presentert i figur [\ref{fig:firstiterationbinary}] er det mye støy i bildet. En smoothingalgoritme ble derfor implementert. Denne algoritmen gikk over det binære bildet og fjernet mindre ansamlinger av punkter og fremhevet de som var større. Dette førte til et renere bilde, med færre og tydeligere objekter. Resultatet av smoothingen er presentert i figur [\ref{fig:seconditerationbinary}].

\begin{figure}[!ht]
	\centering
	\subcaptionbox{Før smoothing\label{fig:firstiterationbinary}}
	{\includegraphics[width=0.45\textwidth]{img/first-binary.jpg}}
	\subcaptionbox{Etter smoothing\label{fig:seconditerationbinary}}
	{\includegraphics[width=0.45\textwidth]{img/second-binary.jpg}}
	\caption{Resultatet av filtrering av bilde før og etter smoothing}
\end{figure}

Smoothingen fremhevet de store objektene i bildet. Dette gjorde det betydelig lettere å fange opp kun det objektet, gitt riktig innstilling av fargefiltrene, da små objekter i bildet med samme farge som objektet ble eliminert.

\newpage
\subsubsection{Brukergrensesnitt}

\begin{wrapfigure}[12]{r}{0.3\textwidth}
	\centering
	\includegraphics[width=\linewidth]{img/command-menu.png}
	\caption{En oversikt over de tilgjengelige kommandoene i kommandolinjegrensesnittet}
	\label{fig:commandmenu}
\end{wrapfigure}

Bestemmelsen av fargene i objektet ble automatisert ved at det ble implementert et enkelt kommandolinjegrensesnitt. Her kunne en med kommandoen \texttt{center} be programmet om å følge etter fargen som befant seg i sentrum av bildet. Grensesnittet er vist i figur [\ref{fig:commandmenu}]. Kommandolinjegrensesnittet ble utvidet etter hvert som flere funksjoner ble implementert i programmet.

For å gjøre det enklere å følge spesifikke objekter i kamerabildet, ble det implementert en funksjon som lot oss klikke på objekter i bildet for å følge dem. Både denne funksjonaliteten og \texttt{center}-kommandoen fungerte slik at fargeverdien til punktet som var i sentrum av bildet eller som ble klikket på ble hentet inn, og at sliderne automatisk ble satt til denne verdien pluss og minus en margverdi. Gode margverdier ble funnet eksperimentelt, og det ble bestemt at en marg på mellom $20$ og $30$ for hver av aksene \emph{hue}, \emph{saturation} og \emph{value} fungerte godt.

\subsection{Valg av hardware}

Det ble først bestemt at Arduino Uno skulle brukes til formålet. Fordelen var at det finnes et standard bibliotek for servoer for Arduino. I utgangspunktet skulle systemet ha et gyrometer som skulle måle orienteringen av riggen og kompensere for dette. På denne måten kunne kameraet stå vinkelrett uansett hvordan flyet bevegde seg. Det ble valgt å benytte seg av en modifisert utgave av biblioteket for gyrometeret MPU-6050 \cite{GyroLib}. Dette viste seg å bli et problem da Arduino ikke støtter multithreading. Det ble konflikt mellom PWM-delen av servobiblioteket og $I^2C$ \cite{i2c} som gyrometeret benytter seg av.

For å løse dette, ble det gjort forsøk på å benytte seg av to Arduinoer. Lesingen av gyrometeret og pådrag til servoene skulle bli utført på ulike arduinoer med UART som kommunikasjonsprotokoll mellom dem. Erfaringene viste at Arduinoen som skulle motta kommandoer mottok mye støy. Pådraget til servoene ble dermed veldig ustabilt.

Det ble da besluttet å gå over til Raspberry Pi. Som presisert i seksjon [\ref{sec:Pi}], har Raspberry Pi mulighet for multithreading. En lignende konflikt mellom PWM og $I^2C$ vil dermed ikke skje. Det oppstod imidlertid utfordringer da Raspberry Pi kun har 1 PWM-port. Det finnes en rekke biblioteker som løser dette ved å genere PWM-signal i programvaren og sende dem gjennom en vanlig digital port. Til dette formålet ble biblioteket WiringPi \cite{WirPi} benyttet. For gyrometeret ble et standard $I^2C$-bibliotek brukt. Det viste seg at konfigurasjonen av WiringPi-biblioteket var tidkrevende, og det ble bestemt å gå tilbake til Arduino, men uten gyrometer.

Denne implementasjonen består av en Arduino som kjører servobiblioteket. Arduinoen får input fra en PC via USB, og gir pådrag til servoene basert på denne. Testene viste at servoene nå hadde en stabil oppførsel, og valget falt derfor på denne implementasjonen.

\subsection{Konstruksjon av riggen}

\subsubsection{Planlegging}
Riggens oppgave er å endre synsretningen til kameraet etter kommando fra bildegjennkjennings-programvaren. Synsvinkelen kan modelleres som en vektor, hvor retningen til vektoren er variabel. Hvis vektoren, $\bf{v}$, settes inn i et koordinatsystem med utspring i origo og sfæriske koordinater, kan situasjonen beskrives som i figur [\ref{fig:spher}].

\begin{figure}[h!]
	\centering
	\includegraphics[scale=0.5]{img/RettVek.jpg}
	\caption{Sfærisk koordinatsystem}
	\label{fig:spher}
\end{figure}

Her er $\phi$ vinkelen mellom x- og y-aksene og $\theta$ er vinkelen mellom z-aksen og xy-planet. Siden riggen skal festes til et fly og kameraet skal se ned på bakken vil det ikke være behov for vinkelretningen å bevege seg inn i den øvre halvdelen av koordinatsystemet. Det betyr at $\phi$ og $\theta$ kun trenger 180 graders utslag for å dekke hele den nedre halvdelen. Dermed kan hele dette området dekkes ved hjelp av to servomotorer med 180 graders utslag, satt sammen som illustrert i figur [\ref{fig:IdeRigg}].

\begin{figure}[h!]
	\centering
	\includegraphics[scale=0.5]{img/BasicRiggIde.png}
	\caption{Servostyring av $\theta$ og $\phi$}
	\label{fig:IdeRigg}
\end{figure}

Kameraet festes til servomotor 2, som beveger seg i retning $\theta$. Samtidig beveger servomotor 1 på servo 2 og kamera i retning $\phi$. For å kunne trekke riggen inn i flyet ved landing ble det bestemt å feste riggen på en bom som kunne heves å senkes ved hjelp av ytterligere en servomotor, som vist i figur [\ref{fig:bom}].

\begin{figure}[h!]
	\centering
	\includegraphics[scale=0.5]{img/Motor3.png}
	\caption{Servomotor 3 kan heve riggen inn i flykroppen.}
	\label{fig:bom}
\end{figure}

\subsubsection{Sammenstilling}

Riggen ble sammenstilt med målene på Kongsbergs prototypefly som utgangspunkt \cite{LocalHawkPDF}. Med disse målene ble det klart at servomotorene måtte være små i størrelse og valget falt på servomotoren HD-1600A. Med sine beskjedene fysiske mål på 21.3x11.6x22.8 mm og en vekt på $6$ g \cite{PowerHD}, ble HD-1600A ansett som perfekt for dette formålet. HS-50 fra Hitec ble også vurdert, men med lavere dreiemoment og betydelig høyere pris ble HD-1600A regnet som et bedre alternativ. For å bygge delene som skulle koble servoene sammen ble det brukt $6$ mm MDF-plater fordi disse er stive og relativt sterke. Dette fører til at de ikke bøyes eller gir etter ved raske rotasjoner. Figur [\ref{fig:RiggTegn}] viser hvordan servomotorene er koblet sammen og figur [\ref{fig:RiggBilde}] viser et bilde av den ferdige prototyperiggen. 

\begin{figure}[h!]
	\centering
	\includegraphics[scale=0.5]{img/RIGG_sattsammen.png}
	\caption{Tegning av mekaniske rigg}
	\label{fig:RiggTegn}
\end{figure}

\begin{figure}[h!]
	\centering
	\includegraphics[scale=0.25]{img/Rigg_Bilde.jpg}
	\caption{Prototyperigg}
	\label{fig:RiggBilde}
\end{figure}

Servomotorene styres av et PWM-signal som genereres av en Arduino UNO. Biblioteket \texttt{Servo.lib} brukes for å kontrollere servoene gjennom tre I/O porter. Servo 1 er koblet til port 9, servo 2 er koblet til port 6 og servo 3 er koblet til port 3. Ved målinger viste det seg at hver servo kan trekke opp mot 250mA under last. Det ble valgt å bruke en ekstern 6V batteripakke med fire seriekoblede 1.5V AA batterier, koblet inn som vist i figur [\ref{fig:ArduSkjem}], i stedet for å la servoene trekke forsyningsstrøm rett fra $5$ V-pinnen til arduinokortet. Arduinokortet bruker strøm fra USB, som ikke kan levere mer enn $500$ mA på grunn av en innebygget sikring i kortet. Dermed kan ikke arduinokortet levere de ca. $750$ mA som trengs når alle servoene går med stor last samtidig.

\begin{figure}[h!]
	\centering
	\includegraphics[scale=0.25]{img/KoblingsskjemaArduino.png}
	\caption{Koblingsskjema for Arduino}
	\label{fig:ArduSkjem}
\end{figure}  

Kommandoer sendes til Arduino via USB-porten. Kommandoer kan sendes som \texttt{Char} på formen \texttt{100a120b40c}, hvor tallene foran \texttt{c} forteller posisjon til servo 1, tallene for \texttt{b} gir posisjon til servo 2 og tallene foran \texttt{c} posisjonenen til servo 3. Dermed vil strengen \texttt{100a120b40c} flytte servo 1 til 100 grader, servo 2 til 120 grader og servo 3 til 40 grader. De gyldige vinklene er fra 0 til 180 grader. Dersom \texttt{200a} sendes vil servo 1 stoppe på 180 grader. 

\subsection{Styring av rigg}

På dette steget av arbeidet var det blitt ferdigstilt en rigg med et kamera som kunne styres med signaler fra en PC, og programvare som kunne detektere et objekt i bildet fra kameraet basert på fargen. For å koble disse to delene sammen, var det nødvendig med en metode for å oversette objektets posisjon i bildet til vinkelkommandoer som kunne sendes til riggen. Dette med mål om å få kameraet til å peke direkte på det detekterte objektet.

Som tidligere beskrevet var den opprinnelige planen å kun bruke to av servomotorene til å styre kameraet, og bruke den tredje kun til å trekke kameraet inn i flyet. Det ble imidlertid klart at det var svært utfordrende å regne ut relasjonen mellom objektets posisjon i bildet og riggens bevegelse dersom disse to servomotorene ble benyttet. Som en enklere løsning ble det valgt å bruke servomotor 1 og 3 til å følge objektet. Med riktig forhåndsinnstilling av servomotor 2 står bevegelsesaksene til servomotor 1 og 3 omtrent normalt på hverandre, slik at de henholdsvis beveger seg langs tenkte x- og y-akser.

Fra kameraleverandøren ble det innhentet informasjon om størrelsen på kameraets synsfelt. Kombinert med det detekterte objektets avstand fra bildesenteret i piksler, ble det mulig å regne ut antall grader riggen måtte bevege seg for å plassere objektet i sentrum av bildet. Ved å legge til riggen nåverende posisjon til dette tallet som lagres hver gang riggen flytter på seg, ble en absolutt posisjon som riggen skulle flytte seg til regnet ut.

Det siste steget var å sende bevegelseskommandoene til Arduino-kortet, som kontrollerte servoene. Opprinnelig ble riggen bedt om å bevege seg hele avstanden i grader mellom sentrum av bildet og objekt for hvert nye bilde fra kameraet.Dette førte til svært stor ustabilitet og oscillasjoner rundt de detekterte objektene. Dette ble løst ved å sette en begrensning på bevegelsen, der riggen aldri ble tillatt å bevege seg mer enn 2 eller 1 grad om gangen, avhengig av hvor nært objektet var sentrum av bildet.

Det ble senere oppdaget at punktet OpenCV beregnet som midten av det detekterte objektet gjerne varierte tilfeldig selv om objektet sto i ro, noe som førte til unødvendige bevegelser av riggen. Det ble dermed spesifisert i koden at dersom objektet var under 3 grader under sentrum, skulle riggen ikke bevege seg. En annen ting som ble bemerket var at motorene ikke opererte perfekt innenfor hele området $0$ til $180$ grader. Servomotor 1 hadde et ideelt bevegelsesområde på omtrent $15$ til $175$ grader, mens servomotor 3 kunne bevege seg mellom rundt $10$ og $140$ grader uten problemer. En sperre ble implementert som gjorde at dersom det var nødvendig for motorene å bevege seg utenfor disse intervallene, ble bevegelsene ikke gjennomført. Med disse begrensningene på riggens bevegelse på plass, fungerte objektfølgingen tilfredsstillende, uten for mye ustabilitet og med grei fart.
